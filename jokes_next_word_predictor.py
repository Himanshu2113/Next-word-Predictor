# -*- coding: utf-8 -*-
"""Jokes next word predictor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DQZUWKE5qyzkfMwGvQqENuhhybddG95o
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d abhinavmoudgil95/short-jokes

import zipfile
zip_ref=zipfile.ZipFile('/content/short-jokes.zip','r')
zip_ref.extractall('/content')
zip_ref.close()

import pandas as pd
df=pd.read_csv('shortjokes.csv')
df = df.drop(columns=['ID'])
df.sample()

df=df.iloc[:100, :]

input=[]

import re

# Remove numeric ID using regex
for i in df['Joke']:
  clean_text = re.sub(r'^\d+\t', '', i)
  input.append(clean_text)
  print(clean_text)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import numpy as np
from tensorflow.keras.utils import Sequence

tokenizer=Tokenizer()
tokenizer.fit_on_texts(input)

tokenizer.word_index

print(len(tokenizer.word_index))

batch_size=50
tokenized_jokes = []

for i in range(0, len(input), batch_size):
    batch = input[i : i + batch_size]  # Get batch
    tokenized_batch = tokenizer.texts_to_sequences(batch)  # Tokenize batch
    tokenized_jokes.extend(tokenized_batch)

tokenized_jokes[0]

class DataGenerator(Sequence):
    def __init__(self, texts, batch_size, max_len, vocab_size):
        self.texts = texts
        # self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.max_len = max_len
        self.vocab_size = vocab_size
        self.indexes = np.arange(len(texts))

    def __len__(self):
        return int(np.ceil(len(self.texts) / self.batch_size))

    def __getitem__(self, idx):
        batch_texts = self.texts[idx * self.batch_size:(idx + 1) * self.batch_size]
        # sequences = self.tokenizer.texts_to_sequences(batch_texts)

        input_sequences = []
        output_words = []

        for seq in tokenized_jokes:
            for i in range(1, len(seq)):
                input_sequences.append(seq[:i])
                output_words.append(seq[i])

        input_sequences = pad_sequences(input_sequences, maxlen=self.max_len, padding='pre')
        output_words = to_categorical(output_words, num_classes=self.vocab_size)

        return np.array(input_sequences), np.array(output_words)

# Parameters
batch_size = 50
max_len = max(len(seq) for seq in (tokenized_jokes))
vocab_size = len(tokenizer.word_index) + 1

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU,Bidirectional

max_len

model = Sequential()
model.add(Embedding(input_dim=773,output_dim=75,input_length=28))
# model.add(LSTM(200,return_sequences=True))
# model.add(LSTM(150,return_sequences=True))
model.add(LSTM(200))
model.add(Dense(773,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

data_generator = DataGenerator(tokenized_jokes, batch_size, max_len, vocab_size)

model.fit(data_generator,epochs=100)

model.fit(data_generator,epochs=100)

text="He"

for i in range(25):
  token_text=tokenizer.texts_to_sequences([text])[0]

  padded_ip=pad_sequences([token_text],maxlen=29,padding='pre')

  p=np.argmax(model.predict(padded_ip))

  for word,index in tokenizer.word_index.items():
    if index==p:
      text=text+" "+word
      print(text)

