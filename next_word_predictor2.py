# -*- coding: utf-8 -*-
"""Next word predictor2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DDVP2SkMtfceswyzsGDoJNYEiIKRGPKE
"""

faqs = """About the Program
What is the course fee for  Data Science Mentorship Program (DSMP 2023)
The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.
What is the total duration of the course?
The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600(approx.)
What is the syllabus of the mentorship program?
We will be covering the following modules:
Python Fundamentals
Python libraries for Data Science
Data Analysis
SQL for Data Science
Maths for Machine Learning
ML Algorithms
Practical ML
MLOPs
Case studies
You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program-637339afe4b0615a1bbed390
Will Deep Learning and NLP be a part of this program?
No, NLP and Deep Learning both are not a part of this program’s curriculum.
What if I miss a live session? Will I get a recording of the session?
Yes all our sessions are recorded, so even if you miss a session you can go back and watch the recording.
Where can I find the class schedule?
Checkout this google sheet to see month by month time table of the course - https://docs.google.com/spreadsheets/d/16OoTax_A6ORAeCg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.
What is the time duration of all the live sessions?
Roughly, all the sessions last 2 hours.
What is the language spoken by the instructor during the sessions?
Hinglish
How will I be informed about the upcoming class?
You will get a mail from our side before every paid session once you become a paid user.
Can I do this course if I am from a non-tech background?
Yes, absolutely.
I am late, can I join the program in the middle?
Absolutely, you can join the program anytime.
If I join/pay in the middle, will I be able to see all the past lectures?
Yes, once you make the payment you will be able to see all the past content in your dashboard.
Where do I have to submit the task?
You don’t have to submit the task. We will provide you with the solutions, you have to self evaluate the task yourself.
Will we do case studies in the program?
Yes.
Where can we contact you?
You can mail us at nitish.campusx@gmail.com
Payment/Registration related questions
Where do we have to make our payments? Your YouTube channel or website?
You have to make all your monthly payments on our website. Here is the link for our website - https://learnwith.campusx.in/
Can we pay the entire amount of Rs 5600 all at once?
Unfortunately no, the program follows a monthly subscription model.
What is the validity of monthly subscription? Suppose if I pay on 15th Jan, then do I have to pay again on 1st Feb or 15th Feb
15th Feb. The validity period is 30 days from the day you make the payment. So essentially you can join anytime you don’t have to wait for a month to end.
What if I don’t like the course after making the payment. What is the refund policy?
You get a 7 days refund period from the day you have made the payment.
I am living outside India and I am not able to make the payment on the website, what should I do?
You have to contact us by sending a mail at nitish.campusx@gmail.com
Post registration queries
Till when can I view the paid videos on the website?
This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased subscription on 21st Jan, you will be able to watch all the past paid sessions in the period of 21st Jan to 20th Feb. But after 21st Feb you will have to purchase the subscription again.
But once the course is over and you have paid us Rs 5600(or 7 installments of Rs 799) you will be able to watch the paid sessions till Aug 2024.
Why lifetime validity is not provided?
Because of the low course fee.
Where can I reach out in case of a doubt after the session?
You will have to fill a google form provided in your dashboard and our team will contact you for a 1 on 1 doubt clearance session
If I join the program late, can I still ask past week doubts?
Yes, just select past week doubt in the doubt clearance google form.
I am living outside India and I am not able to make the payment on the website, what should I do?
You have to contact us by sending a mail at nitish.campusx@gmai.com
Certificate and Placement Assistance related queries
What is the criteria to get the certificate?
There are 2 criterias:
You have to pay the entire fee of Rs 5600
You have to attempt all the course assessments.
I am joining late. How can I pay payment of the earlier months?
You will get a link to pay fee of earlier months in your dashboard once you pay for the current month.
I have read that Placement assistance is a part of this program. What comes under Placement assistance?
This is to clarify that Placement assistance does not mean Placement guarantee. So we dont guarantee you any jobs or for that matter even interview calls. So if you are planning to join this course just for placements, I am afraid you will be disappointed. Here is what comes under placement assistance
Portfolio Building sessions
Soft skill sessions
Sessions with industry mentors
Discussion on Job hunting strategies
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()

tokenizer.fit_on_texts([faqs])

tokenizer.word_index

inputs=[]
for sent in faqs.split('\n'):
  tokenized_sent=tokenizer.texts_to_sequences([sent])[0]

  for i in range(1,len(tokenized_sent)):
    inputs.append(tokenized_sent[:i+1])

inputs

max_len=max([len(x) for x in inputs])
print(max_len)

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_input = pad_sequences(inputs,maxlen=max_len,padding='pre')

X = padded_input[:,:-1]
y = padded_input[:,-1]

X

y

from tensorflow.keras.utils import to_categorical
import numpy as np
y=np.array(y)
y=to_categorical(y,num_classes=283)

y

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

model = Sequential()

model.add(Embedding(input_dim=283,output_dim=100,input_length=56))
model.add(LSTM(150))
model.add(Dense(283,activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

model.fit(X,y,epochs=100)

# prediction
text="deep learning"

for i in range(10):
  token_text=tokenizer.texts_to_sequences([text])[0]

  padded_ip=pad_sequences([token_text],maxlen=56,padding='pre')

  p=np.argmax(model.predict(padded_ip))

  for word,index in tokenizer.word_index.items():
    if index==p:
      text=text+" "+word
      print(text)

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d abhinavmoudgil95/short-jokes

import zipfile
zip_ref=zipfile.ZipFile('/content/short-jokes.zip','r')
zip_ref.extractall('/content')
zip_ref.close()

import pandas as pd
df=pd.read_csv('shortjokes.csv')
df = df.drop(columns=['ID'])
df.sample()

print(type(df))

# df = df.to_frame()
df=df.iloc[:500, :]
# df.shape

df.shape
df.sample(5)

input=[]

import re

# Remove numeric ID using regex
for i in df['Joke']:
  clean_text = re.sub(r'^\d+\t', '', i)
  input.append(clean_text)
  print(clean_text)

input

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer_1=Tokenizer()
tokenizer_1.fit_on_texts(input)

tokenizer_1.word_index

print(len(tokenizer_1.word_index))
# input.shape

batch_size=1000
tokenized_jokes = []

for i in range(0, len(input), batch_size):
    batch = input[i : i + batch_size]  # Get batch
    tokenized_batch = tokenizer_1.texts_to_sequences(batch)  # Tokenize batch
    tokenized_jokes.extend(tokenized_batch)

tokenized_jokes[0]

def process_batch(text_batch, max_len, vocab_size):

    # Convert batch to sequences

    input_sequences = []
    output_words = []

    # Create input-output pairs
    for seq in text_batch:
        for i in range(1, len(seq)):
            input_sequences.append(seq[:i])  # Growing input sequence
            output_words.append(seq[i])  # Next word (target)

    # Pad input sequences
    input_sequences = pad_sequences(tokenized_jokes, maxlen=max_len, padding='pre')

    # One-hot encode output
    output_words = to_categorical(output_words, num_classes=vocab_size)

    return np.array(input_sequences), np.array(output_words)

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import numpy as np
from tensorflow.keras.utils import Sequence

batch_size = 100  # Process 50,000 rows at a time
max_len = max(len(seq) for seq in tokenized_jokes)  # Get max sequence length
vocab_size = len(tokenizer_1.word_index) + 1  # Vocabulary size

# Final lists to store processed data (optional: you can directly train in batches instead)
input_sequences_final = []
output_words_final = []

# Process in batches
for i in range(0, len(tokenized_jokes), batch_size):
    batch_texts = tokenized_jokes[i:i + batch_size]  # Get a batch of data
    input_batch, output_batch = process_batch(batch_texts, max_len, vocab_size)

    input_sequences_final.extend(input_batch)  # Append processed batch
    output_words_final.extend(output_batch)  # Append processed batch

# Convert to NumPy arrays
input_sequences_final = np.array(input_sequences_final)
output_words_final = np.array(output_words_final)

print("Final Input Shape:", input_sequences_final.shape)
print("Final Output Shape:", output_words_final.shape)

class DataGenerator(Sequence):
    def __init__(self, texts, batch_size, max_len, vocab_size):
        self.texts = texts
        # self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.max_len = max_len
        self.vocab_size = vocab_size
        self.indexes = np.arange(len(texts))

    def __len__(self):
        return int(np.ceil(len(self.texts) / self.batch_size))

    def __getitem__(self, idx):
        batch_texts = self.texts[idx * self.batch_size:(idx + 1) * self.batch_size]
        # sequences = self.tokenizer.texts_to_sequences(batch_texts)

        input_sequences = []
        output_words = []

        for seq in tokenized_jokes:
            for i in range(1, len(seq)):
                input_sequences.append(seq[:i])
                output_words.append(seq[i])

        input_sequences = pad_sequences(input_sequences, maxlen=self.max_len, padding='pre')
        output_words = to_categorical(output_words, num_classes=self.vocab_size)

        return np.array(input_sequences), np.array(output_words)

# Parameters
batch_size = 1000
max_len = max(len(seq) for seq in (tokenized_jokes))
vocab_size = len(tokenizer_1.word_index) + 1

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

max_len

model = Sequential()
model.add(Embedding(input_dim=2506,output_dim=100,input_length=38))
model.add(LSTM(150))
model.add(Dense(2506,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

data_generator = DataGenerator(tokenized_jokes, batch_size, max_len, vocab_size)

model.fit(data_generator, epochs=100)

text="mosquito"

for i in range(10):
  token_text=tokenizer_1.texts_to_sequences([text])[0]

  padded_ip=pad_sequences([token_text],maxlen=56,padding='pre')

  p=np.argmax(model.predict(padded_ip))

  for word,index in tokenizer_1.word_index.items():
    if index==p:
      text=text+" "+word
      print(text)

